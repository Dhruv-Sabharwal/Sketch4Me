{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from cnn_helper import cnn_classify\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "Gqc6D2gcHafD",
    "outputId": "5b6328db-54e8-4ee1-e74e-d20c5e1b0c2b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!git clone -l -s https://github.com/Dhruv-Sabharwal/GAN_Face_Creator data\n",
    "%cd data\n",
    "!ls\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPgMpuPtnIy1"
   },
   "source": [
    "# Importing images and image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "EVvA0o-QHi1D",
    "outputId": "a42e0aa4-3c1c-4f06-f545-2d24760c18da"
   },
   "outputs": [],
   "source": [
    "#Reading all the data from CelebA csv file into nparray\n",
    "my_data = genfromtxt(\"celeba-dataset/list_attr_celeba.csv\", delimiter=',',dtype='unicode')\n",
    "list_delt=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41] # columns to be deleted\n",
    "\n",
    "my_data=np.delete(my_data, list_delt, 1)  # deleting columns\n",
    "my_data=np.delete(my_data, 0, 0)  # deleting first row (which contains the names of the columns)\n",
    "\n",
    "# We are going to train the GAN separately for male and female images\n",
    "sorted_array = my_data[np.argsort(my_data[:, 1])]   # sort by column [male = column 1]\n",
    "\n",
    "i_names=[]\n",
    "training_samples=35000\n",
    "\n",
    "# Storing names of images we want in i_names\n",
    "for i in range(training_samples):\n",
    "  i_names.append(sorted_array[i][0])\n",
    "\n",
    "# Storing images in images\n",
    "import imageio\n",
    "data_dir=\"celeba-dataset/img_align_celeba/img_align_celeba\"\n",
    "images=[]\n",
    "for i in i_names:\n",
    "  images.append(imageio.imread(data_dir + '/' + i))\n",
    "\n",
    "# Converting to numpy array\n",
    "images=np.asarray(images)\n",
    "\n",
    "# scale images to preferred size\n",
    "from skimage.transform import resize\n",
    "images_list = list()\n",
    "for image in images:\n",
    "\t# resize with nearest neighbor interpolation\n",
    "\tnew_image = resize(image, (80,80), mode='constant')\n",
    "\timages_list.append(new_image)\n",
    " \n",
    "# Converting to numpy arrays\n",
    "images_list=np.asarray(images_list)\n",
    "dataset=images_list  # dataset now contains all the images required to train our GAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvT9wbdYnFNP"
   },
   "source": [
    "# Defining GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXNWY8FlIJOg"
   },
   "outputs": [],
   "source": [
    "# Defining Discriminator\n",
    "def define_discriminator(in_shape=(80,80,3)):\n",
    "\tmodel=Sequential()\n",
    "\tmodel.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  # downsample to 40x40\n",
    "\tmodel.add(Conv2D(64, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  # downsample to 20x20\n",
    "\tmodel.add(Conv2D(64, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  # downsample to 10x10\n",
    "\tmodel.add(Conv2D(64, (5,5), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  # downsample to 5x5\n",
    "\tmodel.add(Conv2D(64, (5,5), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# classifier\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# Defining Generator\n",
    "def define_generator(latent_dim):\n",
    "\tmodel = Sequential()\n",
    "\t# foundation for 5x5 feature maps\n",
    "\tn_nodes = 64 * 5 * 5\n",
    "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Reshape((5, 5, 64)))\n",
    "\n",
    "\t# upsample to 10x10\n",
    "\tmodel.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\t# upsample to 20x20\n",
    "\tmodel.add(Conv2DTranspose(64, (5,5), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\t# upsample to 40x40\n",
    "\tmodel.add(Conv2DTranspose(64, (7,7), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\t# upsample to 80x80\n",
    "\tmodel.add(Conv2DTranspose(64, (9,9), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\t# output layer 80x80x3\n",
    "\tmodel.add(Conv2D(3, (11,11), activation='tanh', padding='same'))\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# Defining the GAN model\n",
    "def define_gan(g_model, d_model):\n",
    "\t# making weights in the discriminator not trainable\n",
    "\td_model.trainable = False\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(g_model)\n",
    "\tmodel.add(d_model)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# select n real samples from the training dataset\n",
    "def generate_real_samples(dataset, n):\n",
    "  # generate n random numbers\n",
    "  arr = randint(0, len(dataset)-1, n)\n",
    "  # retrieve images from training dataset\n",
    "  X =[]\n",
    "  for i in range(len(arr)):\n",
    "    X.append(dataset[arr[i]])\n",
    "  X=np.asarray(X)\n",
    "  y = ones((n, 1))  # The y label is always 1 for real samples \n",
    "  return X, y\n",
    "\n",
    "# generate n points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "\t# generate points in the latent space (gaussian distribution)\n",
    "\tx_input = randn(latent_dim * n)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# use the generator to generate n synthetic images\n",
    "def generate_fake_samples(g_model, latent_dim, n):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n)\n",
    "\t# predict outputs using generator i.e. generate synthetic images\n",
    "\tX = g_model.predict(x_input)\n",
    "\ty = zeros((n, 1))  # The y label is always 0 for fake samples \n",
    "\treturn X, y\n",
    "\n",
    "# evaluate the discriminator and save periodically save the generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "\t# get real samples from the dataset\n",
    "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "\t# evaluate discriminator on real samples\n",
    "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "\t# use the generator to generate fake samples\n",
    "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# evaluate discriminator on fake examples\n",
    "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "\t# summarize discriminator performance\n",
    "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "\t# save the generator model to Google-Drive\n",
    "\tfilename = '/content/drive/My Drive/GAN-FEMALE3/generator_model_%03d.h5' % (epoch+1)\n",
    "\tg_model.save(filename)\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=350, n_batch=128):\n",
    "\tbat_per_epo = len(dataset) // n_batch\n",
    "\thalf_batch = n_batch // 2\n",
    "\t# iterate over n_epochs epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t# iterate over bat_per_epo batches\n",
    "\t\tfor j in range(bat_per_epo):\n",
    "\t\t\t# get real samples from the dataset\n",
    "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t\t# update discriminator model weights\n",
    "\t\t\td_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n",
    "\t\t\t# generate synthetic samples using the generator\n",
    "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t\t# update discriminator model weights\n",
    "\t\t\td_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "\t\t\t# generate latent points for input to the generator\n",
    "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t\t# y labels as input to the GAN should be 1, so that the generator aims at getting better and fooling the discriminator\n",
    "\t\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t\t# update the generator via the discriminator's error\n",
    "\t\t\tg_loss, g_acc = gan_model.train_on_batch(X_gan, y_gan)  # During GAN model training the discriminator weights are fixed\n",
    "\t\t\t# print loss on this batch\n",
    "\t\t\tprint('>%d, %d/%d, d1=%.3f, a1=%.3f, d2=%.3f, a2=%.3f, g=%.3f, g_acc=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_acc1, d_loss2, d_acc2, g_loss, g_acc))\n",
    "\t\t# evaluate the model performance for every 10 epochs\n",
    "\t\tif (i+1) % 10 == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5QJqaNEnRkF"
   },
   "source": [
    "# Training the GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6U9GVLf-I2cu",
    "outputId": "cf632974-9ac3-4d09-e0cb-70e87c6da11d"
   },
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "tgKM2Rr2ZRAO",
    "outputId": "89c59099-9a75-41af-93a2-00f6d097d662"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from numpy.random import randn\n",
    "\n",
    "model_gen = load_model('generator_model_280.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKiSjgFdfwLq"
   },
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space (Gaussian Distribution)\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the generator\n",
    "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn z_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMpE-2Xuf_JZ"
   },
   "outputs": [],
   "source": [
    "latent_points = generate_latent_points(100, 100)\n",
    "# generate images\n",
    "X_gen  = model_gen.predict(latent_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pDJKKHiDgMPA",
    "outputId": "77f82357-135d-4045-c2e2-0bd4e731594d"
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "  plt.imshow(X_gen[i])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating faces with required facial features\n",
    "gen_features = []\n",
    "my_features = [1, 0 , 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "while(True):\n",
    "    latent_points = generate_latent_points(100, 1)\n",
    "    # generate image\n",
    "    X_gen  = model_gen.predict(latent_points)\n",
    "    gen_features = cnn_classify(X_gen)\n",
    "    if(gen_features==my_features):\n",
    "        print(\"Got required features\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GAN-model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
